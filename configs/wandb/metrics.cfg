[metrics]
default_label = "pile-valppl"
perplexity_default_metric = "valppl"

[metrics.aliases]
pile = "pile-valppl"
pile_val = "pile-valppl"
pile_valppl = "pile-valppl"
pile-valppl = "pile-valppl"
c4 = "c4_en-valppl"
c4_en = "c4_en-valppl"
c4-en = "c4_en-valppl"
m2d2_s2orc = "m2d2_s2orc-valppl"
m2d2-s2orc = "m2d2_s2orc-valppl"
ice = "ice-valppl"
dolma_wiki = "dolma_wiki-valppl"
dolma_stack = "dolma_stack-valppl"
dolma_reddit = "dolma_reddit-valppl"
dolma_pes2o = "dolma_pes2o-valppl"
dolma_common_crawl = "dolma_common-crawl-valppl"
dolma_common-crawl = "dolma_common-crawl-valppl"
dolma_books = "dolma_books-valppl"

[metrics.perplexity_map]
eval/wikitext_103-validation/Perplexity = "wikitext_103-valppl"
eval/pile-validation/Perplexity = "pile-valppl"
eval/c4_en-validation/Perplexity = "c4_en-valppl"
eval/m2d2_s2orc-validation/Perplexity = "m2d2_s2orc-valppl"
eval/ice-validation/Perplexity = "ice-valppl"
eval/dolma_wiki-validation/Perplexity = "dolma_wiki-valppl"
eval/dolma_stack-validation/Perplexity = "dolma_stack-valppl"
eval/dolma_reddit-validation/Perplexity = "dolma_reddit-valppl"
eval/dolma_pes2o-validation/Perplexity = "dolma_pes2o-valppl"
eval/dolma_common-crawl-validation/Perplexity = "dolma_common-crawl-valppl"
eval/dolma_books-validation/Perplexity = "dolma_books-valppl"

[metrics.perplexity_tasks]
names = ["pile", "c4_en", "wikitext_103", "m2d2_s2orc", "ice", "dolma_wiki", "dolma_stack", "dolma_reddit", "dolma_pes2o", "dolma_common-crawl", "dolma_books"]

[metrics.tasks]
olmes = ["mmlu_average", "arc_challenge", "arc_easy", "boolq", "csqa", "hellaswag", "openbookqa", "piqa", "socialiqa", "winogrande"]
mmlu = ["mmlu_abstract_algebra", "mmlu_anatomy", "mmlu_astronomy", "mmlu_average", "mmlu_business_ethics", "mmlu_clinical_knowledge", "mmlu_college_biology", "mmlu_college_chemistry", "mmlu_college_computer_science", "mmlu_college_mathematics", "mmlu_college_medicine", "mmlu_college_physics", "mmlu_computer_security", "mmlu_conceptual_physics", "mmlu_econometrics", "mmlu_electrical_engineering", "mmlu_elementary_mathematics", "mmlu_formal_logic", "mmlu_global_facts", "mmlu_high_school_biology", "mmlu_high_school_chemistry", "mmlu_high_school_computer_science", "mmlu_high_school_european_history", "mmlu_high_school_geography", "mmlu_high_school_government_and_politics", "mmlu_high_school_macroeconomics", "mmlu_high_school_mathematics", "mmlu_high_school_microeconomics", "mmlu_high_school_physics", "mmlu_high_school_psychology", "mmlu_high_school_statistics", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_human_aging", "mmlu_human_sexuality", "mmlu_international_law", "mmlu_jurisprudence", "mmlu_logical_fallacies", "mmlu_machine_learning", "mmlu_management", "mmlu_marketing", "mmlu_medical_genetics", "mmlu_miscellaneous", "mmlu_moral_disputes", "mmlu_moral_scenarios", "mmlu_nutrition", "mmlu_philosophy", "mmlu_prehistory", "mmlu_professional_accounting", "mmlu_professional_law", "mmlu_professional_medicine", "mmlu_professional_psychology", "mmlu_public_relations", "mmlu_security_studies", "mmlu_sociology", "mmlu_us_foreign_policy", "mmlu_virology", "mmlu_world_religions"]

[metrics.metric_names]
names = ["correct_choice", "acc_raw", "acc_per_token", "acc_per_char", "acc_per_byte", "acc_uncond", "no_answer", "sum_logits_corr", "logits_per_token_corr", "logits_per_char_corr", "bits_per_byte_corr", "correct_prob", "correct_prob_per_token", "correct_prob_per_char", "margin", "margin_per_token", "margin_per_char", "total_prob", "total_prob_per_token", "total_prob_per_char", "uncond_correct_prob", "uncond_correct_prob_per_token", "uncond_correct_prob_per_char", "uncond_total_prob", "norm_correct_prob", "norm_correct_prob_per_token", "norm_correct_prob_per_char", "primary_metric"]
