[defaults]
num_finetune_epochs = "1"
initial_checkpoint_steps = "main"
comparison_metric = "pile"

[recipe_mapping]
d17 = "Dolma1.7"
d16 = "Dolma1.6++"
c4 = "C4"
dclm = "DCLM-Baseline"
dclm_qc10p = "DCLM-Baseline (QC 10%)"
dclm_qc20p = "DCLM-Baseline (QC 20%)"
dclm_qc7p_fw2 = "DCLM-Baseline (QC 7%, FW2)"
dclm_qc7p_fw3 = "DCLM-Baseline (QC 7%, FW3)"
dclm_qcfw10p = "DCLM-Baseline (QC FW 10%)"
dclm_qcfw3p = "DCLM-Baseline (QC FW 3%)"
dclm25_dolma75 = "DCLM-Baseline 25% / Dolma 75%"
dclm50_dolma50 = "DCLM-Baseline 50% / Dolma 50%"
dclm75_dolma25 = "DCLM-Baseline 75% / Dolma 25%"
dclm_25_d17_75 = "DCLM-Baseline 25% / Dolma 75%"
dclm_50_d17_50 = "DCLM-Baseline 50% / Dolma 50%"
dclm_75_d17_25 = "DCLM-Baseline 75% / Dolma 25%"
falcon = "Falcon"
falcon_cc = "Falcon+CC"
falcon_cc_qc10p = "Falcon+CC (QC 10%)"
falcon_cc_qc20p = "Falcon+CC (QC 20%)"
falcon_cc_qcorig10p = "Falcon+CC (QC Orig 10%)"
falcon_cc_qctulu10p = "Falcon+CC (QC Tulu 10%)"
fineweb_edu = "FineWeb-Edu"
fineweb_pro = "FineWeb-Pro"
dolma1_7 = "Dolma1.7"
dclm_qc7fw2 = "DCLM-Baseline (QC 7%, FW2)"
dclm_qc7fw3 = "DCLM-Baseline (QC 7%, FW3)"
dclm-baseline = "DCLM-Baseline"
dclm-baseline-qc-10p = "DCLM-Baseline (QC 10%)"
dclm-baseline-qc-20p = "DCLM-Baseline (QC 20%)"
dclm-baseline-qc-7p-fw2 = "DCLM-Baseline (QC 7%, FW2)"
dclm-baseline-qc-7p-fw3 = "DCLM-Baseline (QC 7%, FW3)"
dclm-baseline-qc-fw-10p = "DCLM-Baseline (QC FW 10%)"
dclm-baseline-qc-fw-3p = "DCLM-Baseline (QC FW 3%)"
"Dolma 1.7" = "Dolma1.7"
"Dolma 1.6" = "Dolma1.6++"
"C4" = "C4"
"DCLM" = "DCLM-Baseline"
"DCLM Baseline" = "DCLM-Baseline"
"Falcon" = "Falcon"
"FineWeb" = "FineWeb-Edu"

[patterns]

[patterns.ft1]
run_type = "simple_ft_vary_tokens"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_BLOCK_STEPS_WORD}_{INITIAL_CHECKPOINT_STEPS_WORD}_{FINETUNE_TOKENS_EPOCHS_8}{LEARNING_RATE_FLAG}$"

[patterns.ft3]
run_type = "simple_ft_vary_tokens"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_BLOCK_STEPS_WORD}_{INITIAL_CHECKPOINT_STEPS_WORD}_{FINETUNE_TOKENS_8}_toks{LEARNING_RATE_FLAG}$"

[patterns.ft4]
run_type = "simple_ft"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_BLOCK_STEPS_WORD}_Ft{LEARNING_RATE_FLAG}$"

[patterns.ft5]
run_type = "simple_ft"
regex = "^{TIMESTAMP_6_EXP_NAME}_DD-{INITIAL_CHECKPOINT_RECIPE_DASH}-{INITIAL_CHECKPOINT_SIZE}_Ft{LEARNING_RATE_EQUAL}$"

[patterns.ft6]
run_type = "simple_ft_vary_tokens"
regex = "^{TIMESTAMP_6_EXP_NAME}_{FINETUNE_TOKENS_EPOCHS_8}_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.ft7]
run_type = "simple_ft"
regex = "^{TIMESTAMP_6_EXP_NAME}_Ft_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.matched1]
run_type = "matched"
regex = "^{MATCHED_PREFIX_WITH_METRIC}{FINETUNE_TOKENS_6}_{DD_BLOCK_FULL}$"

[patterns.matched2]
run_type = "matched"
regex = "^{MATCHED_PREFIX_WITH_METRIC}{FINETUNE_FT}_{DD_BLOCK_FULL}$"

[patterns.matched3]
run_type = "matched"
regex = "^{MATCHED_PREFIX_6}{FINETUNE_FT}_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.matched4]
run_type = "matched"
regex = "^{MATCHED_PREFIX_6}{FINETUNE_TOKENS_6}_{DD_BLOCK_FULL}$"

[patterns.matched5]
run_type = "matched"
regex = "^{MATCHED_PREFIX_6}{FINETUNE_FT}_{DD_BLOCK_FULL}$"

[patterns.matched6]
run_type = "matched"
regex = "^{TIMESTAMP_6_EXP_NAME}_{DD_COMPARISON_6}_Ft_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.matched7]
run_type = "matched"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_COMPARISON_6}_Ft_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.matched8]
run_type = "matched"
regex = "^{MATCHED_PREFIX_6}{FINETUNE_TOKENS_6}_{DD_BLOCK_FULL}{LR_SUFFIX}$"

[patterns.reduce_loss]
run_type = "reduce_type"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_BLOCK_STEPS_WORD}_{INITIAL_CHECKPOINT_STEPS_WORD}_default_--max_train_samples={FINETUNE_TOKENS_SIMPLE}_--reduce_loss={REDUCE_LOSS}$"

[patterns.dpo1]
run_type = "dpo"
regex = "^{TIMESTAMP_8_EXP_NAME}_{DD_BLOCK_STEPS_WORD}_{INITIAL_CHECKPOINT_STEPS_WORD}_default$"

[patterns.dpo2]
run_type = "dpo"
regex = "^{TIMESTAMP_8_EXP_NAME}_dd__{INITIAL_CHECKPOINT_RECIPE}-{INITIAL_CHECKPOINT_SIZE}__{INITIAL_CHECKPOINT_STEPS_WORD}__{FINETUNE_TOKENS_GT}_lr={LEARNING_RATE_1}_default_--learning_rate={LEARNING_RATE_2}$"

[processing]

[processing.column_renames]
initial_checkpoint_recipe = "ckpt_data"
initial_checkpoint_size = "ckpt_params"
initial_checkpoint_steps = "ckpt_steps"

[processing.fill_from_config]
lr = "learning_rate"
seed = "seed"
num_finetune_epochs = "num_train_epochs"

[processing.run_type_hooks]
matched = "dr_ingest.wandb.hooks:normalize_matched"
